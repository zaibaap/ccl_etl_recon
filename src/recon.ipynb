{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6ac2dbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openpyxl in c:\\Users\\zaiba\\Desktop\\ccl_etl_recon\\.venv\\Lib\\site-packages (3.1.5)\n",
      "Requirement already satisfied: et-xmlfile in c:\\Users\\zaiba\\Desktop\\ccl_etl_recon\\.venv\\Lib\\site-packages (from openpyxl) (2.0.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e78f692",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\zaiba\\Desktop\\ccl_etl_recon\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['Company Cashbook', 'Bank Statement', 'Bank Reconciliation Statement', 'W-1 (BS to CB)', 'W-2 (CB to BS)'])\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import unicodedata\n",
    "\n",
    "from typing import Dict, List\n",
    "from pathlib import Path\n",
    "\n",
    "# ensure project root is on sys.path for imports when running in a notebook\n",
    "ROOT = Path.cwd().resolve().parents[0]\n",
    "if str(ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(ROOT))\n",
    "\n",
    "from src.DataCleaner import DataCleaner\n",
    "\n",
    "data_path = ROOT / \"data\" / \"1. Bank Reconciliation Sample.xlsx\"\n",
    "\n",
    "sheets = pd.read_excel(data_path, sheet_name=None, header=4)\n",
    "print(sheets.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "140bfd91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18, 6) (12, 8)\n",
      "Index(['Unnamed: 0', 'Date', 'Particulars', 'Debit', 'Credit', 'Balance'], dtype='object')\n",
      "Index(['Unnamed: 0', 'Date', 'Details', 'Amount ($)', 'Unnamed: 4', 'Date.1',\n",
      "       'Details.1', 'Amount ($).1'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "df_bs = pd.read_excel(data_path, sheet_name=\"Bank Statement\", header=5)\n",
    "df_cb = pd.read_excel(data_path, sheet_name=\"Company Cashbook\", header=4)\n",
    "\n",
    "print(df_bs.shape, df_cb.shape)\n",
    "print(df_bs.columns)\n",
    "print(df_cb.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e6330fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_empty_unnamed(df):\n",
    "    \"\"\"\"   \n",
    "    drop unnamed columns if they are fully empty\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    unnamed = [c for c in df.columns if str(c).startswith(\"Unnamed\")]\n",
    "    df = df.drop(columns=[c for c in unnamed if df[c].isna().all()], errors=\"ignore\")\n",
    "    return df\n",
    "\n",
    "def clean_cols(df):\n",
    "    \"\"\"\n",
    "    Clean/ standardize column names\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    df.columns = (\n",
    "        df.columns.astype(str)\n",
    "        .str.strip()\n",
    "        .str.replace(\"\\n\", \" \", regex=False)\n",
    "        .str.replace(\" \", \"_\")\n",
    "        .str.replace(\".\", \"_\")\n",
    "        .str.upper()\n",
    "    )\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4d642172",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['DATE', 'PARTICULARS', 'DEBIT', 'CREDIT', 'BALANCE'], dtype='object')\n",
      "Index(['DATE', 'DETAILS', 'AMOUNT_($)', 'DATE_1', 'DETAILS_1', 'AMOUNT_($)_1'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "df_bs = drop_empty_unnamed(df_bs)\n",
    "df_cb = drop_empty_unnamed(df_cb)\n",
    "\n",
    "df_bs = clean_cols(df_bs)\n",
    "df_cb = clean_cols(df_cb)\n",
    "\n",
    "print(df_bs.columns)\n",
    "print(df_cb.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7ae6c9cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_cb we see DATE and DATE_1 because the data is split into two section - debit and credit\n",
    "# the last row of each section is the total\n",
    "# we need to split into debit and credit, and remove the total \n",
    "\n",
    "\n",
    "def split_debit_credit(df):\n",
    "    df = df.copy()\n",
    "\n",
    "    if \"DATE_1\" in df.columns:\n",
    "        debit_cols = [\"DATE\", \"DETAILS\", \"AMOUNT_($)\"]\n",
    "        credit_cols = [\"DATE_1\", \"DETAILS_1\", \"AMOUNT_($)_1\"]\n",
    "        debit_rename = {\"AMOUNT_($)\": \"AMOUNT\"}\n",
    "        credit_rename = {\"DATE_1\": \"DATE\", \"DETAILS_1\": \"DETAILS\", \"AMOUNT_($)_1\": \"AMOUNT\"}\n",
    "\n",
    "\n",
    "    debit = df[debit_cols].rename(columns=debit_rename).copy()\n",
    "    credit = df[credit_cols].rename(columns=credit_rename).copy()\n",
    "\n",
    "    # standardize names for rest of pipeline\n",
    "    debit.columns = [\"DATE\", \"DETAILS\", \"AMOUNT\"]\n",
    "    credit.columns = [\"DATE\", \"DETAILS\", \"AMOUNT\"]\n",
    "\n",
    "    return debit, credit\n",
    "\n",
    "def drop_total_and_blank_rows(df):\n",
    "    df = df.copy()\n",
    "\n",
    "    df = df.dropna(how=\"all\")\n",
    "\n",
    "    # drop \"total\" rows based on DETAILS text\n",
    "    if \"DETAILS\" in df.columns:\n",
    "        mask_total = df[\"DETAILS\"].astype(str).str.contains(r\"\\btotal\\b\", case=False, na=False)\n",
    "        df = df.loc[~mask_total]\n",
    "\n",
    "    return df.reset_index(drop=True)\n",
    "\n",
    "\n",
    "# next we should clean all the columns of data to ensure that they are processed and the right types assigned\n",
    "cleaner = DataCleaner()\n",
    "\n",
    "COLUMN_CLEANERS = {\n",
    "    \"DATE\": cleaner.apply_date,\n",
    "    \"DETAILS\": cleaner.apply_text,\n",
    "    \"PARTICULARS\": cleaner.apply_text,\n",
    "    \"AMOUNT\": cleaner.apply_number,\n",
    "    \"DEBIT\": cleaner.apply_number,\n",
    "    \"CREDIT\": cleaner.apply_number,\n",
    "    \"BALANCE\": cleaner.apply_number,\n",
    "}\n",
    "\n",
    "def apply_cleaner(df):\n",
    "    df = df.copy()\n",
    "\n",
    "    for col, clean_func in COLUMN_CLEANERS.items():\n",
    "        if col in df.columns:\n",
    "            df[col] = clean_func(df[col])  \n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6e076bea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zaiba\\Desktop\\ccl_etl_recon\\src\\DataCleaner.py:153: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  return pd.to_datetime(x, errors=\"coerce\", infer_datetime_format=True)\n",
      "C:\\Users\\zaiba\\Desktop\\ccl_etl_recon\\src\\DataCleaner.py:153: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  return pd.to_datetime(x, errors=\"coerce\", infer_datetime_format=True)\n",
      "C:\\Users\\zaiba\\Desktop\\ccl_etl_recon\\src\\DataCleaner.py:153: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  return pd.to_datetime(x, errors=\"coerce\", infer_datetime_format=True)\n"
     ]
    }
   ],
   "source": [
    "debit, credit = split_debit_credit(df_cb)\n",
    "\n",
    "debit = drop_total_and_blank_rows(debit)\n",
    "credit = drop_total_and_blank_rows(credit)\n",
    "\n",
    "# the opening row of debit and closing row of credit have balances that need to be removed\n",
    "# the credit and debit databases shpuld be transactions only \n",
    "# we can save them seperately if needed later\n",
    "\n",
    "cb_opening_bal = debit.loc[debit[\"DETAILS\"] == \"Balance b/d\"]\n",
    "cb_closing_bal = credit.loc[credit[\"DETAILS\"] == \"Balance c/d\"]\n",
    "bs_opening_bal = df_bs.loc[df_bs[\"PARTICULARS\"] == \"Opening Balance\"]\n",
    "\n",
    "debit = debit.drop(cb_opening_bal.index)\n",
    "credit = credit.drop(cb_closing_bal.index)\n",
    "df_bs = df_bs.drop(bs_opening_bal.index)\n",
    "\n",
    "\n",
    "# Now apply the cleaner\n",
    "debit = apply_cleaner(debit)\n",
    "credit = apply_cleaner(credit)\n",
    "df_bs = apply_cleaner(df_bs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "780631aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we can create a unified cashbook dataframe by combining debit and credit\n",
    "# ! this is a cash account where debit increases cash and credit decreases \n",
    "\n",
    "cb_debit = debit.copy()\n",
    "cb_debit[\"TYPE\"] = \"RECIEPT\"\n",
    "cb_debit[\"SIGNED_AMOUNT\"] = cb_debit[\"AMOUNT\"]\n",
    "\n",
    "cb_credit = credit.copy()\n",
    "cb_credit[\"TYPE\"] = \"PAYMENT\"\n",
    "cb_credit[\"SIGNED_AMOUNT\"] = -cb_credit[\"AMOUNT\"]\n",
    "\n",
    "df_cb_unified = (\n",
    "    pd.concat([cb_debit, cb_credit], ignore_index=True)\n",
    "      .sort_values(\"DATE\")\n",
    "      .reset_index(drop=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fdadcae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine debit and credit into signed amount in bank statement\n",
    "df_bs[\"SIGNED_AMOUNT\"] = (\n",
    "    df_bs[\"CREDIT\"].fillna(0)\n",
    "    - df_bs[\"DEBIT\"].fillna(0)\n",
    ")\n",
    "\n",
    "# standardize column names -> particulars = details in df_bs\n",
    "df_bs = df_bs.rename(columns={\n",
    "    \"PARTICULARS\": \"DETAILS\"\n",
    "})\n",
    "\n",
    "# we want to split the details and the transaction nums to make matching easier\n",
    "# extract trailing transaction number (if present)\n",
    "df_bs[\"TRANSACTION_NO\"] = df_bs[\"DETAILS\"].str.extract(r\"(\\d+)$\")\n",
    "df_bs[\"DETAILS_TEXT\"] = df_bs[\"DETAILS\"].str.replace(r\"\\s*\\d+$\", \"\", regex=True)\n",
    "\n",
    "df_cb_unified[\"TRANSACTION_NO\"] = df_cb_unified[\"DETAILS\"].str.extract(r\"(\\d+)$\")\n",
    "df_cb_unified[\"DETAILS_TEXT\"] = df_cb_unified[\"DETAILS\"].str.replace(r\"\\s*\\d+$\", \"\", regex=True)\n",
    "\n",
    "\n",
    "df_bs[\"DETAILS\"] = df_bs[\"DETAILS_TEXT\"]\n",
    "df_bs.drop(columns=\"DETAILS_TEXT\", inplace=True)\n",
    "\n",
    "df_cb_unified[\"DETAILS\"] = df_cb_unified[\"DETAILS_TEXT\"]\n",
    "df_cb_unified.drop(columns=\"DETAILS_TEXT\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "430a7af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure all the transaction number column is type int\n",
    "df_bs[\"TRANSACTION_NO\"] = cleaner.apply_trans_num(df_bs[\"TRANSACTION_NO\"])\n",
    "df_cb_unified[\"TRANSACTION_NO\"] = cleaner.apply_trans_num(df_cb_unified[\"TRANSACTION_NO\"])\n",
    "\n",
    "df_bs[\"DETAILS\"] = cleaner.apply_text(df_bs[\"DETAILS\"])\n",
    "df_cb_unified[\"DETAILS\"] = cleaner.apply_text(df_cb_unified[\"DETAILS\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "200b7635",
   "metadata": {},
   "outputs": [],
   "source": [
    "# start with unified cashbook + cleaned bank statement\n",
    "df_cb_remaining = df_cb_unified.copy()\n",
    "df_bs_remaining = df_bs.copy()\n",
    "\n",
    "# keep row ids for removal after matching\n",
    "df_cb_remaining = df_cb_remaining.reset_index(drop=False).rename(columns={\"index\": \"CB_ROWID\"})\n",
    "df_bs_remaining = df_bs_remaining.reset_index(drop=False).rename(columns={\"index\": \"BS_ROWID\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3b777705",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_match = df_cb_remaining.merge(\n",
    "    df_bs_remaining,\n",
    "    on=[\"TRANSACTION_NO\", \"SIGNED_AMOUNT\"],\n",
    "    how=\"inner\",\n",
    "    suffixes=(\"_cb\", \"_bs\")\n",
    ")\n",
    "\n",
    "# remove matched rows from remaining datasets\n",
    "matched_cb_ids = df_match[\"CB_ROWID\"].unique()\n",
    "matched_bs_ids = df_match[\"BS_ROWID\"].unique()\n",
    "\n",
    "df_cb_remaining = df_cb_remaining[~df_cb_remaining[\"CB_ROWID\"].isin(matched_cb_ids)].reset_index(drop=True)\n",
    "df_bs_remaining = df_bs_remaining[~df_bs_remaining[\"BS_ROWID\"].isin(matched_bs_ids)].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de1d12fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_match.drop(columns=[\"DEBIT\", \"CREDIT\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5824751e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# since this data set is simple, we have matched all possible rows\n",
    "# in real world data sets, I would add more matching logic here based on fuzzy matching of details and date ranges \n",
    "# and combinations of signed amounts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "34b01c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#processing for business insight:\n",
    "\n",
    "df_cb_remaining[\"RECON_CATEGORY\"] = np.where(\n",
    "    df_cb_remaining[\"TYPE\"] == \"PAYMENT\",\n",
    "    \"Unpresented payment\",\n",
    "    np.where(\n",
    "        df_cb_remaining[\"TYPE\"] == \"RECIEPT\",\n",
    "        \"Outstanding deposit\",\n",
    "        \"Other reconciling item\"\n",
    "    )\n",
    ")\n",
    "\n",
    "def classify_bs(details):\n",
    "    details = str(details).upper()\n",
    "    if \"BANK CHARGES\" in details:\n",
    "        return \"Bank charges not recorded\"\n",
    "    elif \"DIVIDEND\" in details:\n",
    "        return \"Direct income not recorded\"\n",
    "    elif \"DIRECT DEBIT\" in details:\n",
    "        return \"Direct debit not recorded\"\n",
    "    else:\n",
    "        return \"Other reconciling item\"\n",
    "    \n",
    "df_bs_remaining[\"RECON_CATEGORY\"] = df_bs_remaining[\"DETAILS\"].apply(classify_bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ffabcc08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    True\n",
       "Name: BALANCE, dtype: bool"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check final reconcialiation proof\n",
    "\n",
    "# --- Bank Statment\n",
    "sum_matched = cleaner.apply_number(df_match[\"SIGNED_AMOUNT\"]).sum() \n",
    "sum_bs_unmatched = cleaner.apply_number(df_bs_remaining[\"SIGNED_AMOUNT\"]).sum()\n",
    "\n",
    "bs_opening_bal = bs_opening_bal[\"BALANCE\"]\n",
    "\n",
    "bs_check = bs_opening_bal + sum_matched + sum_bs_unmatched\n",
    "\n",
    "# check if the sum of the opening balance, the sum of all matche dtransactions, and the sum of all unmatched\n",
    "# are equal to the closing bank balance\n",
    "\n",
    "bs_check == df_bs[\"BALANCE\"].iloc[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5697d0a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zaiba\\AppData\\Local\\Temp\\ipykernel_17316\\2745611192.py:6: FutureWarning: Calling float on a single element Series is deprecated and will raise a TypeError in the future. Use float(ser.iloc[0]) instead\n",
      "  closing_cash = float(cb_closing_bal[\"AMOUNT\"])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    True\n",
       "Name: AMOUNT, dtype: bool"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- Company cashbook\n",
    "\n",
    "sum_cb_unmatched = cleaner.apply_number(df_cb_remaining[\"SIGNED_AMOUNT\"]).sum()\n",
    "cb_check = cb_opening_bal[\"AMOUNT\"] + sum_cb_unmatched + sum_matched\n",
    "\n",
    "closing_cash = float(cb_closing_bal[\"AMOUNT\"])\n",
    "\n",
    "cb_check == closing_cash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dd079a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cb_unified.to_csv((ROOT / \"data\") / \"df_cb_unified.csv\", index=False)\n",
    "df_match.to_csv((ROOT / \"data\") / \"df_matched.csv\", index=False)\n",
    "df_cb_remaining.to_csv((ROOT / \"data\") / \"df_cb_remaining.csv\", index=False)\n",
    "df_bs_remaining.to_csv((ROOT / \"data\") / \"df_bs_remaining.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
